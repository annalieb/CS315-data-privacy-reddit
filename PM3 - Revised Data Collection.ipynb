{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae392c4",
   "metadata": {},
   "source": [
    "## PM3 - Revised Data Collection\n",
    "5/2/2022  \n",
    "Anna Lieb\n",
    "\n",
    "In my previous data collection notebook for PM2, I used the Pushshift API to collect Reddit data from the r/technology subreddit. However, my search was too narrow and did not generate enough text data for thorough analysis. \n",
    "\n",
    "In this notebook, I will use downloaded Reddit dumps from https://files.pushshift.io/reddit/submissions/ and https://files.pushshift.io/reddit/comments/ to widen my search to all subreddits instead. To get relevant submissions, I will still slightly narrow my search to posts that contain the words (\"data\" and \"privacy\") OR (\"data\" and \"personal\"). Note that the search does not consider these terms as bigrams; instead, it searches for both words individually.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Helper functions](#sec1)\n",
    "2. [Collect submissions](#sec2)\n",
    "3. [Collect comments](#sec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa9995",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## 1. Helper functions\n",
    "This section includes helper functions that decompress the Reddit dump .zst file, determines whether a post is \"relevant\" (ie. contains keywords), and writes relevant Reddit posts to an output .csv file.\n",
    "\n",
    "Reddit posts from the .zst file are represented as objects. Some of the object attributes used in this code are listed below. \n",
    "\n",
    "Useful attributes: \n",
    "- author ==> username of the author of the post\n",
    "- subreddit ==> subreddit forum where the post was posted\n",
    "- title (submission only) ==> title of submissions\n",
    "- selftext (submission only) ==> body text of submissions; often, Reddit users leave this blank.\n",
    "- body (comment only) ==> text of comment\n",
    "- created_utc ==> time of post in utc time\n",
    "- stickied ==> boolean for whether the post was pinned to top of thread\n",
    "- id ==> post id\n",
    "- permalink ==> link to the post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0d255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for file management\n",
    "import zstandard as zstd # enables parsing of .zst files\n",
    "import pandas as pd # for data handling and storage\n",
    "import datetime # for UTC parsing\n",
    "import csv\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa0db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeFn(entry, textAttr):\n",
    "    '''\n",
    "        Determines whether or not an entry should be discarded \n",
    "        based on a set of conditional checks.\n",
    "        \n",
    "        return True ==> exclude\n",
    "        return False ==> include\n",
    "        \n",
    "        Parameters: \n",
    "        entry - Pushshift object that represents the post\n",
    "        textAttr - 'body' for comments and 'title' for posts \n",
    "    '''\n",
    "    valueof = lambda key: entry.get(key, \"\")\n",
    "    \n",
    "    text = valueof(textAttr) # textAttr =\"body\" for comments, \"title\" for submissions\n",
    "    \n",
    "    # do not include if post has been removed\n",
    "    if (text in ['', '[deleted]', '[removed]']): \n",
    "        return True\n",
    "    \n",
    "    # do not include if post is \"stickied\", ie. pinned to top of thread\n",
    "    if (+valueof('stickied')): \n",
    "        return True\n",
    "    \n",
    "    # must be in given subreddit\n",
    "    #if (valueof('subreddit').lower() in ['amitheasshole']): \n",
    "    #    return False\n",
    "    \n",
    "    if (\"data\" in text) and ((\"privacy\" in text) or (\"personal\" in text)): \n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2b456",
   "metadata": {},
   "source": [
    "### extractZst\n",
    "Based on code snippets posted in this Reddit thread: https://www.reddit.com/r/pushshift/comments/ajmcc0/information_and_code_examples_on_how_to_use_the/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20308af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractZst(inPath, outPath, attributes, excludeFn = lambda x: False):\n",
    "    '''\n",
    "        Decompresses the given Reddit dump, reads from it as a stream, \n",
    "        and continuously writes relevant posts to an output .csv file. \n",
    "        \n",
    "        Parameters: \n",
    "        inPath ==> file path to input .zst file\n",
    "        outPath ==> file path to output .csv file \n",
    "        attributes ==> attributes of the post objects that you wish to include \n",
    "        in the output file\n",
    "            *For my purposes I used the following attributes: \n",
    "            [\"created_utc\", \"author\", \"title\", \"selftext\", \"subreddit\", \"id\", \"permalink\"] for submissions\n",
    "            [\"created_utc\", \"author\", \"body\", \"subreddit\", \"id\", \"permalink\"] for comments\n",
    "            *Note that the text attribute should be the third item.\n",
    "        excludeFn ==> predicate function to determine relevance of post\n",
    "    '''\n",
    "    print(f'decompressing {inPath}...')\n",
    "    textAttr = attributes[2] # 'body' for comments, 'title' for submissions\n",
    "    \n",
    "    with open(inPath, 'rb') as fh:\n",
    "        # iterate through zst contents via a filestream to minimize memory load\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            previous_line = \"\"\n",
    "            while True:\n",
    "                chunk = reader.read(2**24)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                else:\n",
    "                    string_data = chunk.decode('utf-8')\n",
    "                    lines = string_data.split(\"\\n\")\n",
    "                    \n",
    "                    \n",
    "                    for i, line in enumerate(lines[:-1]):\n",
    "                        if i == 0:\n",
    "                            line = previous_line + line\n",
    "                        \n",
    "                        # entry is the Reddit post as an object\n",
    "                        entry = json.loads(line)\n",
    "                            \n",
    "                        if excludeFn(entry, textAttr):\n",
    "                            pass\n",
    "                        else:\n",
    "                            \n",
    "                            # write to file \n",
    "                            with open(outPath, \"a\") as outF: \n",
    "                                writer = csv.writer(outF)\n",
    "                                row = []\n",
    "                                for attr in attributes: \n",
    "                                    row.append(entry[attr])\n",
    "                                writer.writerow(row)\n",
    "                            \n",
    "                        previous_line = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevantZstToCsv(inPath, outPath, attributes, excludeFn): \n",
    "    '''\n",
    "    Takes a .zst file and writes the relevant posts to a .csv file.\n",
    "    \n",
    "    inpath ==> file path to input .zst \n",
    "    outPath ==> file path to output .csv\n",
    "    attributes ==> list of desired post attrbutes to be included in output csv\n",
    "    excludeFn ==> function that returns boolean value based on post relevance\n",
    "    '''\n",
    "    if os.path.exists(inPath) and not os.path.exists(outPath):\n",
    "        with open(outPath, \"w\") as outF: \n",
    "            writer = csv.writer(outF)\n",
    "    \n",
    "            # header of output csv file \n",
    "            writer.writerow(attributes)\n",
    "            \n",
    "        extractZst(inPath, outPath, attributes, excludeFn)\n",
    "    \n",
    "    else: \n",
    "        print(\"Path error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663a1e6",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## 2. Collect submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission attributes to be collected\n",
    "subAttributes = [\"created_utc\", \"author\", \"title\", \"selftext\", \"subreddit\", \"id\", \"permalink\"]\n",
    "\n",
    "# collect submissions from March 2021 to June 2021\n",
    "for month in range (3, 7): \n",
    "    subInPath = f\"Data.nosync/Dumps/RS_2021-0{month}.zst\"\n",
    "    subOutPath = f\"Data.nosync/Extracted/RS_2021-0{month}_extracted.csv\"\n",
    "    relevantZstToCsv(subInPath, subOutPath, subAttributes, excludeFn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a1a32",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## 3. Collect comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ea927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment attributes to be collected\n",
    "commAttributes = [\"created_utc\", \"author\", \"body\", \"subreddit\", \"id\", \"permalink\"]\n",
    "\n",
    "# collect comments from March 2021 to June 2021\n",
    "for month in range (3, 7): \n",
    "    commInPath = f\"Data.nosync/Dumps/RC_2021-0{month}.zst\"\n",
    "    commOutPath = f\"Data.nosync/Extracted/RC_2021-0{month}_extracted.csv\"\n",
    "    relevantZstToCsv(commInPath, commOutPath, commAttributes, excludeFn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
